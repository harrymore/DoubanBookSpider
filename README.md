# DoubanBookSpider
抓取豆瓣的书籍信息并持久化到mysql数据库

这个爬虫的思路主要受@lanbing510 的启发，不同的是实现方面我自己用了requests库,持久化用了mysql，另外添加了日志模块logging。在应对豆瓣反爬虫方面，由于不想买收费代理，而我自己又是ADSL拨号上网，所以一旦检测到豆瓣返回403，就主动断开路由连接，重连获取新的ip。
以下介绍一下思路和实现。

## 一、爬虫思路及架构
通过观察豆瓣网书籍的具体页面，我们可以发现，具体书籍网址的组成形式为：
https://book.douban.com/subject/bookid/，
其中bookid为具体的数字。第一种思路是设定一个比较大的数字，然后从1到这个数字的范围之内去遍历所有数字对应的网址，但是我们可以发现，这些书的id往往非常大，基本都是百万级别的数字，一个个去撞库非常不现实。
其实每本书都有很多标签，每个标签都汇集了同一类的所有书，要是可以获取到所有标签，然后根据这些标签一个个去遍历就可以获得豆瓣上所有的书了，当然不同标签之间肯定有重复的书，所以还要考虑去重的问题。
所以，这个爬虫的思路主要是：
1. 爬取总标签页（https://book.douban.com/tag/?view=cloud）获取所有标签，并持久化；
2. 通过上一步获取的标签，组成标签页的网址，进入标签页获得本页的所有书籍链接，通过书籍链接获取需要信息；
3. 对书籍信息进行标准化，持久化处理；
4. 爬完本标签，切到下一个标签，继续爬，重复2,3步；
5. 应对网站反爬虫机制；
6. 错误日志和程序崩溃处理。
编程语言选择python，持久化工具选择mysql数据库。

## 二、数据库设计
1. 标签信息
表名为tag_info，用来保存从豆瓣爬下来的标签名字，另外，增加多两个字段用来表示进度，一个是当前页数，另一个是完成状态。
2. 书籍信息：
表名为book_info，保存了每本书的基本信息，包括书名、作者、出版社、评分、评分人数等。为了去重，唯一id采用了豆瓣网中书籍的id。 

## 三、爬虫框架及流程分析
在这个爬虫中使用的模块有 requests，BeautifulSoup，re，logging和MySQLdb，主要流程如下图所示。

![](https://github.com/harrymore/DoubanBookSpider/blob/master/images/框架.jpg)

 1. 用requests模块向豆瓣网发起GET请求，获得网页返回的信息；
2. 把上一步返回的信息用BeautifulSoup模块进行分析，如果还需要进一步匹配过滤，传给re模块进一步加工，否则直接传给MySQLdb模块；
3. re模块对BeautifulSoup传过来的信息进行匹配处理，最后传给MySQLdb； 
4. 用MySQLdb连接mysql数据库，将获得的书籍信息持久化到mysql中；
5. 用logging模块负责系统日志的输出和维护。

## 四、应对豆瓣的反爬虫
实际上这次爬取数据，比计划多花了一倍时间，大部分时间在于应对豆瓣的反爬虫机制。
刚开始requests去请求数据的时候，准备了若干个header，然后每次请求都换一个。没过多久就开始返回403了，于是减低了爬取的频率，还是继续403。
查了半天资料，首次请求保存cookie，以后每次都修改cookiebid去请求，过一段时间还是被豆瓣检测出来了，有的页面直接返回空的或者定向到别的页面。
最后想到了代理，写了一个动态获取免费代理并验证的函数，不过免费的ip代理不仅慢而且不稳定，又不想买收费的代理服务。
后面查到了ADSL拨号服务器代理的相关文章，发现自己正是用ADSL上网，于是折腾了一番，写了一个断开路由器连接的函数，每当爬虫被豆瓣封杀的时候，就断开路由器连接重新获取ip，由此解决了爬虫正常运行的问题。当然，我这次爬取的数据量比较少，所以用这种方式还是能解决问题，如果是需要在短时间获取大量数据的，还是需要用代理的方式。

![](https://github.com/harrymore/DoubanBookSpider/blob/master/images/流程.jpg)

## 五、操作步骤
1. 在你的数据库执行init.sql里的语句创建相应的表；
2. 运行douban_tags.py获取所有标签；
3. 根据自己的情况（上网方式，路由型号，账号密码）修改douban_books.py里关于断线重新disconnect_router()里的信息;
4. 运行douban_books.py开始爬取信息

## 六、完成爬虫
花了几天时间，最后终于完成了所有标签的爬取工作，爬到的数据去重后有6万条左右。因为各个标签之间肯定有重叠的部分，所以符合原来的预期。
写爬虫还是挺有意思的，有时间再写一个爬全部书籍信息的爬虫。

